{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and desired features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring function \n",
    "**How we select a model and how this reflects the model put into practice:**  \n",
    "Ultimately, we care only that we correctly identify failed inspections among the inspections that the City of Chicago actually has the resources to carry out. So, we'd like our model to produce probabilities of passing or not passing -- the inspectors can then inspect establishments with highest probailities of not passing. We'd also like to punish or penalize the model for sending inspectors to inspect establishments that pass but that we predict not to pass with high certainty (or vice versa) (i.e. we'd like to be less wrong when we're not right). \n",
    "\n",
    "The log loss is a suitable objective function to optimize then. The log loss is the negative log likelihood of a Bernoulli random variable (in the 2-class setting, we'll justify this shortly): $$-\\frac{1}{n}\\sum_1^n y_i \\log(p_i)-(1-y_i)\\log(1-p_i)$$ for $n$ observations, where the $i$th observation is of correct class $y_i \\in \\{0,1\\}$ which our model predicts with probability $p_i$. This achieves specifically this sort of penality (suppose $y_i=1$ and we predict this with probability of only $0.1$, this yields a value that approaches $-\\infty$ rapidly).\n",
    "\n",
    "When put into practice, this approach ranks inspections by probability of not passing, so inspectors can carry out inspections that appear at the top of this ranking. Selecting a model with minimal log loss (or maximum likelihood) essentially ensures this ranking is best, that inspectors have the highest chance of inspecting establishments that have commited a violation. Ultimately, in a given time frame with constrained resources, the City of Chicago cannot carry out all the inspections it has to do. There is a cutoff, so they will carry out the inspections at the top of this ranking. \n",
    "\n",
    "How can we get confidence in this approach (what does it look like in practice)? Suppose this model is run only one time (that's the best we can do, we don't have anymore data) and we use the resulting ranking to allocate our inspectors to inspections for $n$ inspections (we take $n$ to be the number of inspections the inspectors were able to do in, say, a month). We then compare the number of failures correctly classified in the top $n$ inspections in the ranking (since there are what would've been done using our best model and methodology) to the number of failures actually found in this same timeframe. If it is greater, then we should be confident in how the model is allocating the City of Chicago's resources. (N.B. of course, the same number of failures will all be discovered over time, but we care about early intervention from a public health perspective!).\n",
    "\n",
    "Also note that a 2-class approach is sufficient here - we care about failing (vs. not failing). 'Pass' vs. 'Pass w/ Conditions' is an unimportant distinction for how this model is motivated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest \n",
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoost\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/data_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to access the year conveniently\n",
    "dataset['inspection_date'] = pd.to_datetime(dataset['inspection_date'])\n",
    "dataset['inspection_date'] = [day.date() for day in dataset['inspection_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-04\n",
      "2016-12-02\n"
     ]
    }
   ],
   "source": [
    "print dataset['inspection_date'].min()\n",
    "print dataset['inspection_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONT RUN THIS, adjusted 0, 1 labeling up above\n",
    "# swap 0s and 1s to make neg log loss scoring functions more easy to interpret (we care about fails)\n",
    "# dataset.replace(to_replace = {'result_binary': {0: 'not pass', 1: 'pass'}}, inplace = True)\n",
    "# dataset.replace(to_replace = {'result_binary': {'not pass': 1, 'pass': 0}}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confirm this \n",
    "# dataset.loc[0:10, ['result_binary', 'results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2010, 2011, 2012, 2013, 2014, 2015, 2016])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(map(lambda d: d.year, dataset['inspection_date'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# withold 2010 for lookback period to build previous features\n",
    "# train on 2011-2015\n",
    "train = dataset[map(lambda d: (d.year != 2010) & (d.year != 2016), dataset['inspection_date'].values)] \n",
    "# test on 2016 - not this notebook, this is Luke\n",
    "test = dataset[map(lambda d: d.year == 2016, dataset['inspection_date'].values)] \n",
    "\n",
    "x_train = train.drop(['result_binary', 'results', 'inspection_date'], axis = 1) # do we want inspection_date anymore? annoying with sklearn functions...\n",
    "y_train = train['result_binary']\n",
    "x_test = test.drop(['result_binary', 'results', 'inspection_date'], axis = 1) \n",
    "y_test = test['result_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120304, 93) (87180, 93) (15742, 93)\n",
      "n.b.: not including observations from 2010\n"
     ]
    }
   ],
   "source": [
    "print dataset.shape, train.shape, test.shape\n",
    "print 'n.b.: not including observations from 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count pass: 68510\n",
      "Count not pass: 18670\n"
     ]
    }
   ],
   "source": [
    "# correct class imbalance in the train data\n",
    "print 'Count pass:', sum(y_train == 0)\n",
    "print 'Count not pass:', sum(y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18646092,  2.19330602,  0.19956804, ...,  0.22694735,\n",
       "        0.22694735, -0.28743414])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.scale(x_train['crime'], copy = False)\n",
    "preprocessing.scale(x_train['sanit'], copy = False)\n",
    "preprocessing.scale(x_test['crime'], copy = False)\n",
    "preprocessing.scale(x_test['sanit'], copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## subsampling algorithm \n",
    "# takes x_train, y_train and returns a class-balanced x_train, y_train \n",
    "# assumes count of class 0 > count of class 1\n",
    "# subsampled down so a bunch of data disappears - we can change this if we want \n",
    "def subsample(x_train, y_train):\n",
    "    train = pd.concat([x_train, y_train], axis = 1)\n",
    "\n",
    "    train_0 = train[y_train == 0]\n",
    "    train_1 = train[y_train == 1]\n",
    "    \n",
    "    train_0_subsample = train_0.sample(train_1.shape[0])\n",
    "    \n",
    "    train_subsample = pd.concat([train_0_subsample, train_1], axis = 0)\n",
    "    \n",
    "    x_train_subsample = train_subsample.iloc[:, :-1]\n",
    "    y_train_subsample = train_subsample.iloc[:, -1]\n",
    "    \n",
    "    return x_train_subsample, y_train_subsample\n",
    "    \n",
    "x_train_sub, y_train_sub = subsample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count pass: 18670\n",
      "Count not pass: 18670\n"
     ]
    }
   ],
   "source": [
    "print 'Count pass:', sum(y_train_sub == 0)\n",
    "print 'Count not pass:', sum(y_train_sub == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train_sub\n",
    "y_train = y_train_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train for visualization without selection by log loss -- produced for poster -- skip below for selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_hats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# true is list of true classes, pred is list of predicted class probabilities \n",
    "def score(y, p_hat):\n",
    "    p_hat = map(lambda t: t[1], p_hat) # p_hat returned from sklearn is a list of lists with p for both classes, we want p for class 1 or fail \n",
    "    epsilon = 1e-15\n",
    "    p_hat = sp.maximum(epsilon, p_hat)\n",
    "    p_hat = sp.minimum(1-epsilon, p_hat)\n",
    "    logloss = sum(y*sp.log(p_hat) + sp.subtract(1,y)*sp.log(sp.subtract(1,p_hat)))\n",
    "    logloss = logloss * -1.0/len(y)\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_model(clf_instance):\n",
    "    clf_instance.fit(x_train, y_train)\n",
    "    print 'accuracy:', round(clf_instance.score(x_test, y_test), 2)\n",
    "    p_hat = clf_instance.predict_proba(x_test)\n",
    "\n",
    "    print '\\n', 'Confusion matrix:'\n",
    "    conf = confusion_matrix(y_test, clf_instance.predict(x_test))\n",
    "    conf = conf / float(conf.sum())\n",
    "    print conf\n",
    "\n",
    "    print 'log loss:', round(score(y_test, p_hat), 2)\n",
    "    return map(lambda t: t[1], p_hat) # p_hat returned from sklearn is a list of lists with p for both classes, we want p for class 1 or fail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logreg\n",
    "logreg = LogReg(C = 1.0, class_weight = 'balanced') \n",
    "naive_model(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = map(lambda x: 10**x, np.linspace(-2, 2, 5))\n",
    "logreg_hyperparams = {'C': alphas}  \n",
    "logreg_labels = ['LogReg, C = 0.10','LogReg, C = 0.1','LogReg, C = 1','LogReg, C = 10','LogReg, C = 100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, alpha in enumerate(alphas):\n",
    "    logreg = LogReg(C = alpha, class_weight = 'balanced') \n",
    "    p_hats[logreg_labels[i]] = naive_model(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda\n",
    "lda = LDA() \n",
    "naive_model(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shrinkage_params = np.linspace(0.0, 1.0, 5)\n",
    "# lda_hyperparams = {'shrinkage': shrinkage}  \n",
    "lda_labels = ['LDA, shrinkage = 0.0','LDA, shrinkage = 0.25','LDA, shrinkage = 0.50','LDA, shrinkage = 0.75','LDA, shrinkage = 1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, shrinkage_param in enumerate(shrinkage_params):\n",
    "    lda = LDA(shrinkage = shrinkage_param, solver = 'lsqr') \n",
    "    p_hats[lda_labels[i]] = naive_model(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# qda\n",
    "qda = QDA() \n",
    "naive_model(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_params = np.linspace(0.0, 1.0, 5)\n",
    "qda_labels = ['QDA, reg = 0.0','QDA, reg = 0.25','QDA, reg = 0.50','QDA, reg = 0.75','QDA, reg = 1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, reg_param in enumerate(reg_params):\n",
    "    qda = QDA(reg_param = reg_param) \n",
    "    p_hats[qda_labels[i]] = naive_model(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "knn = KNN(n_neighbors = 5)\n",
    "naive_model(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns = [5,10,25,50]\n",
    "knn_labels = ['KNN, n = 5','KNN, n = 10','KNN, n = 25','KNN, n = 50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, n in enumerate(ns):\n",
    "    knn = KNN(n_neighbors = n) \n",
    "    p_hats[knn_labels[i]] = naive_model(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bagging - rf\n",
    "# non-cross validated\n",
    "rf = RandomForest()\n",
    "naive_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape[1] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validated\n",
    "rf_hyperparams = {'n_estimators': [25,50],\n",
    "                  'max_features': [x_train.shape[1] / 2, x_train.shape[1]], \n",
    "                  'max_depth': [2, 4]}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfs = [(25,x_train.shape[1] / 2),\n",
    "       (25,x_train.shape[1]), \n",
    "       (50,x_train.shape[1] / 2), \n",
    "       (50,x_train.shape[1])]\n",
    "\n",
    "rf_labels = ['RF, n = 25, features = 45','RF, n = 25, features = 90','RF, n = 50, features = 45','RF, n = 50, features = 90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, rf in enumerate(rfs):\n",
    "    rf = RandomForest(n_estimators = rf[0], max_features = rf[1]) \n",
    "    p_hats[rf_labels[i]] = naive_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# boosting - adaboost\n",
    "# non-cross validated\n",
    "ada = AdaBoost()\n",
    "naive_model(ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adas = [25,50,100]\n",
    "\n",
    "ada_labels = ['AdaBoost, n = 25','AdaBoost, n = 50','AdaBoost, n = 100',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, ada in enumerate(adas):\n",
    "    adaboost = RandomForest(n_estimators = ada) \n",
    "    p_hats[ada_labels[i]] = naive_model(adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_hats['inspection_date'] = test['inspection_date']\n",
    "p_hats['result_binary'] = test['result_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save this down\n",
    "output = pd.DataFrame(p_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv('./data/pred_prob_for_fails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# svm \n",
    "# non-cross validated\n",
    "svc = SVC(probability = True)\n",
    "pred_prob_svc = naive_model(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SLOW\n",
    "# grid CV\n",
    "Cs = map(lambda t: 10**t, range(0, 4))\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "svc_hyperparams = {'C': Cs, 'kernel': kernels}\n",
    "\n",
    "svc = SVC()\n",
    "gs_svc = GridSearchCV(svc, param_grid = svc_hyperparams)\n",
    "gs_svc.fit(x_train, y_train)\n",
    "p_hat = gs_clf.best_estimator_.predict_proba(x_test)\n",
    "\n",
    "print score(y_test, p_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2. Tune multiple model classes by cross validating on log loss -- for final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = map(lambda x: 10**x, np.linspace(-4, 4, 9))\n",
    "logreg_hyperparams = {'C': alphas}  \n",
    "\n",
    "shrinkage_params = np.linspace(0.0, 1.0, 5)\n",
    "lda_hyperparams = {'shrinkage': shrinkage_params, 'solver': ['lsqr']}  \n",
    "\n",
    "reg_params = np.linspace(0.0, 1.0, 5)\n",
    "qda_hyperparams = {'reg_param': reg_params}\n",
    "\n",
    "knn_hyperparams = {'n_neighbors': [5,10,25,50]}\n",
    "\n",
    "rf_hyperparams = {'n_estimators': [25,50],\n",
    "                  'max_features': [x_train.shape[1] / 2, x_train.shape[1]], \n",
    "                  'max_depth': [2, 4]}  \n",
    "\n",
    "ada_hyperparams = {'n_estimators': [25,50,100]} \n",
    "\n",
    "model_classes = [LogReg, LDA, QDA, KNN, RandomForest, AdaBoost]\n",
    "model_classes_str = ['LogReg', 'LDA', 'QDA', 'KNN', 'RandomForest', 'AdaBoost']\n",
    "model_class_hyperparams = [logreg_hyperparams, lda_hyperparams, qda_hyperparams, knn_hyperparams, rf_hyperparams, ada_hyperparams]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return best predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_hats_cv = {} # contains predicted probabilities for each of best models per model class \n",
    "clfs = [] # contains best model per model class\n",
    "\n",
    "for i, model_class in enumerate(model_classes):\n",
    "    gs_clf = GridSearchCV(model_class(), param_grid = model_class_hyperparams[i], scoring = 'neg_log_loss', cv = 5)\n",
    "    gs_clf.fit(x_train, y_train)\n",
    "    best_clf = gs_clf.best_estimator_\n",
    "    \n",
    "    score = 'log loss: ' + str(round(-1 * gs_clf.best_score_, 2))\n",
    "    \n",
    "    # best predictor label - class, score, and the tuned hyperparameters of interest \n",
    "    best_pred_label = model_classes_str[i] + ', ' + score + ', params = ' + \\\n",
    "                    repr([key + ': ' + str(gs_clf.best_estimator_.get_params()[key]) for key in model_class_hyperparams[i].keys()])\n",
    "    \n",
    "    # best predictor's predicted probabilities for class 1 fail \n",
    "    best_pred_prob = map(lambda t: t[1], best_clf.predict_proba(x_test))    \n",
    "    \n",
    "    # store best predictor's probabilities \n",
    "    p_hats_cv[best_pred_label] = best_pred_prob\n",
    "    \n",
    "    # store classifiers as well\n",
    "    clfs.append(best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search selected models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"KNN, log loss: 17.24, params = ['n_neighbors: 50']\",\n",
       " \"RandomForest, log loss: 0.94, params = ['n_estimators: 50', 'max_features: 45', 'max_depth: 2']\",\n",
       " \"QDA, log loss: 1.07, params = ['reg_param: 0.25']\",\n",
       " \"AdaBoost, log loss: 0.73, params = ['n_estimators: 100']\",\n",
       " \"LogReg, log loss: 0.75, params = ['C: 1.0']\",\n",
       " \"LDA, log loss: 0.72, params = ['shrinkage: 0.0', 'solver: lsqr']\"]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Grid search selected models:'\n",
    "p_hats_cv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_hats_cv['inspection_date'] = test['inspection_date']\n",
    "p_hats_cv['result_binary'] = test['result_binary']\n",
    "output = pd.DataFrame(p_hats_cv)\n",
    "output.to_csv('./data/pred_prob_for_fails_cv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.10000000000000001, 1.0, 10.0, 100.0, 1000.0, 10000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='neg_log_loss',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with this \n",
    "# gs_clf = GridSearchCV(LogReg(), param_grid = logreg_hyperparams, scoring = 'neg_log_loss', cv = 5)\n",
    "# gs_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use to get best n classifiers rather than best classifier \n",
    "#\n",
    "# takes a fit grid searched classifier and returns best n classifiers' hyperparameters\n",
    "# def best_params(gs_clf, n): \n",
    "#     best_ids = np.array(map(lambda t: t[1], np.array(gs_clf.grid_scores_))).argsort()[::-1][:n]\n",
    "#     best_params = [gs_clf.grid_scores_[best_id][0] for best_id in best_ids]\n",
    "#     return best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
