<!DOCTYPE html>
<html lang="en-us">
  
  <head>
  <meta charset="UTF-8">
  <title>Optimizing Food Inspections in Chicago</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/foodinspections/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/foodinspections/css/cayman.css">

    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

</head>

  <body>
    <section class="page-header">
  <h1 class="project-name">Optimizing Food Inspections in Chicago</h1>
  <h2 class="project-tagline">A Final Project for CS109a at Harvard University</h2>
  <a href="http://github.com/fggw/foodinspections/" class="btn">View on GitHub</a>
</section>

    <section class="main-content">
      
      






<center> <a href="/foodinspections/2016/12/12/data-exploration.html"> &#8592; Data Exploration</a> | <a href="/foodinspections/2016/12/12/results.html"> &#8594;</a> </center>



<h1>Modeling</h1>

<p>After developing some intuition with exploratory
analysis, we moved on to the modeling process.</p>

<p>On the surface, it seemed that this was a simple problem of building a best predictor. Given some data, can we classify an inspection as a sure fail. We took this approach at first, building a baseline and several tuned models to make classfications. But accuracy, especially accuracy on true fails, was intractable and tought to improve.</p>

<p>Prediction, however, was not the problem here. The methodology by which models were used and not the models themselves is key here: we wish to send inspectors to the most likely fails first and less likely fails as they become available. This sort of ‘ranking’ could be captured in the log loss criterion (the negative log likelihood in a 2-class or Bernoulli setting):</p>

<script type="math/tex; mode=display">l(p) = -\frac{1}{n} \sum y_i \log(p_i) - (1-y_i)\log(1 - p_i)</script>

<p>This choice was appropriate because log loss 
penalizes a probability’s distance from truth, rather than
a prediction’s distance from truth. For example,log loss strongly 
penalizes a case in which a model gave an establishment a 
high probability of passing an inspection when it actually 
ends up failing.</p>

<p>Since this was more nuanced than a problem of prediction, an appropriate scoring approach (e.g. log loss) was more important than selecting a single optimal model. We considered several classes of classifiers - logistic regression, linear and quadratic discriminant analysis, bagging approaches like random forests, boosting approaches like AdaBoost, and so on - but let the data decide which would lend itself best to the task. To select a single ‘best’ model per class, we ran 5-fold cross-validation to search exhaustively over and score by log loss a range of reasonable sets of hyper-parameters, ultimately arriving at a best model per model class.</p>


<br>


<center> <a href="/foodinspections/2016/12/12/data-exploration.html"> &#8592; Data Exploration</a> | <a href="/foodinspections/2016/12/12/results.html"> &#8594;</a> </center>






      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000/foodinspections/">Optimizing Food Inspections in Chicago</a> was created for CS109a by: 
    <ul> 
    
        <li><a href="http://no.website">Luke Farewell</a></li>
    
        <li><a href="http://no.website">Jake Gober</a></li>
    
        <li><a href="http://sa.muel.green">Samuel Green</a></li>
    
        <li><a href="http://no.website">Jeremy Welborn</a></li>
    
    </ul>

  <span class="site-footer-owner">We were advised by <a href="https://github.com/tnames">Taylor Names.
</footer>


    </section>

  </body>
</html>
