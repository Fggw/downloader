---
layout: post
title:  "Model Evaluation and Results"
published: true
page: 6
---

While we found it more reliable to cross-validation using `log-loss`, we evaluated models
using a metric that we called `average fail days`, which more closely approximates how the 
models might be used in practice. 

The goal was to understand the reduction in the number of days until establishments with
violations are caught. To do this, we simulated the inspection process under the ranking determined by our models' recommended inspection orders and compared them with the actual inspections made in 2016.

The metric used to compare our model to the actual inspection results is `average fail days`. This is 
the average number of days it takes to inspect business that fail inspections, measured from the day 
they enter the universe of establishments that can possibly be inspected. `average fail days` 
is an appropriate model of effectiveness in practice, but it wasn't appropriate for cross validation
because there isn't a sense of "ground truth." When employed in practice, the model can recommend
which inspections that take place, whereas in cross-validation, we have a fixed set of inspections
that occurred and can't move them around without losing meaning. 

So, in evaluating effectiveness of the models, we treated 2016 as a pool of flexible possible 
inspections and compared the models' recommended order of inspections, in `average fail days`,
to the true `average fail days` that actually achieved by the Department of Public Health 
through November of 2016. 

This chart below shows how each of the model classes, selected for optimality in the previous
cross-validation process, performed on the test set: 

![Performance]({{site.baseurl}}/images/AverageFailDays.png)

Consistent with our ex-ante anticipation of strong non-linearity, AdaBoost achieves
the largest reduction in the number of fail days compared to the actual inspections
that took place. 

This is further confirmed by examining `rolling average failure rate`. We chose
a performance window of `n` inspections in which to measure the rate of failures discovered.
We chose `n = 40` to approximate the number of inspections per day made by the department in practice.
Results from this are shown below:

![Rolling]({{site.baseurl}}/images/RollingFailRate.png)

As we can see above, AdaBoost is most effective at making failure rates highest at the 
beginning of the window, meaning that it identifies the most failures, and then the lowest
rate of failures at the end, meaning that it has eliminated the most failures from the 
testing pool already. This is what we would hope to observe in the testing phrase, though
in practice we would hope to see that the model identifies a higher rate of failure at a 
consistent rate. This visualization and simulation process supports the conclusion
that our model would result in an improvement over the City's current process. 

Ultimately, we satisfied the objective we set out to accomplish: we were able to output a ranked list of 
establishments for inspectors to inspect, in order, such that the number of days it takes them to 
inspect a failed establishment is lower on average than if they were randomly inspecting restaurants.  
Not only did we beat the baseline random inspection order by about 40 failure days, but we also beat the 
model that the City of Chicago implemented.  In fact, they claimed to have improved inspection order by 
7 failure days.  Our model was thus 33 failure days or so better than Chicagoâ€™s model. We realize that 
this strong of an improvement is almost too good--this means that there are likely issues hidden in out 
model or over process.

Questions that would remain would be: how frequently should the model re-rank possible
inspections? How many inspections should be optimally recommended at a time? 


