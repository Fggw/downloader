---
layout: post
title:  "Model Evaluation, Results, and Conclusion"
published: true
page: 6
---

While we found it more reliable to cross-validation using `log-loss`, we evaluated models
using a metric that we called `average fail days`, which more closely approximates how the 
models might be used in practice. 

The goal was to understand the reduction in the number of days until establishments with
violations are caught. To do this, we simulated the inspection process under the ranking determined by our models' recommended inspection orders and compared them with the actual inspections made in 2016.

The metric used to compare our model to the actual inspection results is `average fail days`. This is 
the average number of days it takes to inspect business that fail inspections, measured from the day 
they enter the universe of establishments that can possibly be inspected. `average fail days` 
is an appropriate model of effectiveness in practice, but it wasn't appropriate for cross validation
because there isn't a sense of "ground truth." When employed in practice, the model can recommend
which inspections that take place, whereas in cross-validation, we have a fixed set of inspections
that occurred and can't move them around without losing meaning. 

So, in evaluating effectiveness of the models, we treated 2016 as a pool of flexible possible 
inspections and compared the models' recommended order of inspections, in `average fail days`,
to the true `average fail days` that actually achieved by the Department of Public Health 
through November of 2016. 

This chart below shows how each of the model classes, selected for optimality in the previous
cross-validation process, performed on the test set: 

![Performance]({{site.baseurl}}/images/AverageFailDays.png)

Consistent with our ex-ante anticipation of strong non-linearity, AdaBoost achieves
the largest reduction in the number of fail days compared to the actual inspections
that took place. 

This is further confirmed by examining `rolling average failure rate`. We chose
a performance window of `n` inspections in which to measure the rate of failures discovered.
We chose `n = 40` to approximate the number of inspections per day made by the department in practice.
Results from this are shown below:

![Rolling]({{site.baseurl}}/images/RollingFailRate.png)

As we can see above, AdaBoost maximizes failure rates
at the beginning of the simulation, compared to 
other models. It also minimizes the failure rate
at the end, meaning that it has eliminated the most 
failures from the 
testing pool already. 

This is what we would hope to observe in the testing phrase, though
in practice we would hope to see that the model identifies a higher rate of failure at a 
consistent rate. This visualization and simulation process supports the conclusion
that our model would result in an improvement over the City's current process. 

AdaBoost reduced the number of days it 
took to inspect establishments that failed by 44 days, at 
108 vs. 152 in the actual 2016 data.

## Possible Drawbacks & Flaws

The 
simulation has several drawbacks could overstate the 
apparent superiority of the model. 

First, the simulation assumes 
that on the first day of 2016 all inspections that will be 
made during 2016 are fully known to the model. In
practice,  inspections that result from complaints and 
food poisoning incidents are not and cannot be known in 
advance. Thus, the universe of 
observable future inspections should be limited. 

Furthermore, we know 
from analyzing the inspections of 2016 that the City of 
Chicago completes on average 69 inspections per day.  so 
The rate at which new inspections arrive should therefore 
be on this order of magnitude. Without more knowledge of 
the inspection discovery process it is impossible to build 
a simulation whose results would be an "apples to apples" 
comparison with the actual inspections from 2016. 
This problem can be thought of as follows: what our
ranking model does is shift failed inspections forward in 
time within the time interval being considered. If 2016 is 
split into many small inspection intervals, each with 
their own limited information regarding future 
inspections, the overall forward shift of failures is 
small. Thus, the performance of the model depends on the 
size of the lookahead time interval. The problem is 
further complicated by the fact that different types of 
inspections with different failure rates have different 
lookahead times.

In addition, a true "average fail days" metric should be 
calculated from a start date that indicates the first day 
a restaurant could have failed an inspection in order to 
reflect the actual number of days that consumers were at 
risk. In order to calculate this metric we would need 
know when establishments qualify for inspection.

## Conclusion

Given these considerations, the true test for our ranking 
model is to run it in practice, as the order in which 
inspections appear and are performed has an impact on 
which inspections are made in the future.

Nonetheless, we believe that our results show promise
and indicate that there is still room for improvement
in optimizing the City's inspection process using
analytics. 

 


