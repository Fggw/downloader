---
layout: post
title:  "Modeling"
published: true
page: 5
---

We began our modeling process with a simple averaging model as our baseline.  This model simply used 
the fraction of past inspection failures for given establiments to predict whether or not it would fail 
in the future.  Using only one predictors, we expected this model to be only so predictive.  We were 
right in that expectation: the model performed at a rate of 71% accuracy.  While this may see reasonable 
at first, it is actually worse than predicting all inspections to be passed.  This is because, given 
that  only `15-20% of establishments fail their inspections in a given year, a model predicting all 
fails would have an accuracy of between 80 and 85%.  Thus, we needed to move on to testing different 
models, each of which would predict on the basis of many variables in the data.  However, before moving 
on to more complicated modelling processes, we reset our goals for what a good model would output. The 
following describes this thought process: 


At first, it seemed that building a model that could take in the attributes of an establishment and 
predict whether or not it would fail was the optimal goal. However, we realized that, given the nature 
of the problem we are trying to solve, this objective was not the best approach.  Because we ultimately 
are trying to reduce the amount of time it takes for an inspector to find a failed establishment, and 
because we want inspectors to inspect establishments in order of most-likely-to-fail to 
least-likely-to-fail, it would be more effective if our model was able to rank establishments.  Thus, we 
set out to find a method to best produce such a list.


We decided to use log loss to select best performing models.  Log loss served our process of model scoring best given our objectives for the following reasons: log loss is designed to penalize the probability of a binary option (in our case, the probability of an establishment failing rather than passing) strongly if the probability is high for, say, 1, when the result ends up being a 0.  In our case, this would mean that log loss would strongly penalize a case in which a model gave an establishment a high probability of passing an inspection when it actually ends up failing. Because the most important aspect of our objective is to put establishments most likely to fail at the top of the inspection order list, this penalization scheme imbedded in log loss was an excellent match for our project goals.

$$ x = 5 $$

While log loss was used to rank models, ranging from AdaBoost to KNN, with different parameters, we then had to adopt a different strategy for selecting the final model.

Given our objective of the project, we decided to select models, via cross validation, based on a metric 
we call “failure days.”  Failure days represent the aggregated number of days it takes, from the start 
of the first inspection in a given time frame, to inspect (and fail) a health-violating establishment.  
For illustration, note the following simple example.  Imagine an inspector is inspecting 10 restaurants 
in the first week of february.  It turns out that 2 of the establishments end up failing while the rest 
pass.  If the inspector inspected the failed establishments on day 2 and day 3 of the week, the failure 
days would be 2 + 3 = 5 failure days.  However, if the inspector inspected the failed establishments on 
day 4 and day 6 of the week, the failure days would be 4 + 6 = 10.  Thus, in this simple case, we would 
select the model that resulted in 5 failure days rather than the one that resulted in 10 failure days.  
By choosing the model that minimizes failure days,we get a ranked list that enables inspectors to 
inspect, in a given time frame, establishments likely to fail sooner than ones likely to pass.


One consideration that proved important in the model building process was the following:  one has to 
think about how often inspectors should receive a new list of ranked establishments to inspect.  In 
other words, should inspectors receive a new ranked list of establishments every week, month, or three 
months?  Is there an optimal time interval for which a new list should be generated? These were 
considerations we had to take into account in the model scoring phase of our process.  We ended up not 
picking a regeneration interval for the ranked lists--rather, we just used log loss to rank the entire 
test set at once.  In future projects, it may be telling to test different list regeneration intervals 
to see if our overall results could be improved.  

In summary, in the model scoring phase, our models feed lists of probabilities to a log loss 
function--these probabilities represent the likelihood of an establishment to fail based on a set of 
different predictors.  Log loss then scores the models, strongly penalizing cases of low probability of 
failure if the result of the inspection turns out to be actually be a failure. Each inspection in the 
list is then officially assigned a “1” or “0” for “pass” or “fail.” Finally, we use these lists or 
ordered passes and fails to measure the number of failure days (using cross validation) of each model, 
which is the aggregate number of days from the first day of inspections to each failed inspection. 


