---
layout: post
title:  "Imputing Missing Values"
published: true
page: 3
---

The dataset - collated over several sources - was rich with upwards of 120,000 observations and 64 features. Based on our exploration and domain expertise with the data, we reduced the features to the 40 or so most meaningful. 

Still, the split of complete observations (no missing values) to incomplete observations (missing values) was roughly one to one. (50% of observations had NaN values.) So a substantial imputation was required. We could be confident the imputed data would be reliable -- 50% of observations had missing values but only 9 or 10% of values were missing. 

![Test]({{site.baseurl}}/images/imputation_exploration.png)

The approach we took was imputation by mean (for quantitative features) or mode (for categorical features) of the nearest neighbor observations. After appropriately encoding categorical features and cleaning up other features on an ad hoc basis, we split the data into a 'train set' corresponding to observations with no missing values and a 'test set' corresponding to observations with one or more missing values. 

In this k-NN imputer, the predictors corresponded to features that were filled in for all observations and the response being predicted corresponded to features that were not yet filled in for all observations. Rather than taking an approach of arbitrarily selecting features to fill and then adding this column to the imputer to impute the remaining features, we started with a set of full features (having filled some in using aggregations like mean or mode for a few with very few NaN values) and used these to compute, for each observation in the test set, the `k = 5` nearest neighbors in the train set. Then for each incomplete feature, we used the values from the nearest neighbors to impute. The code for the final few steps is commented (overly commented) here: 


{% highlight python %}
# for each observation with incomplete features
for i, row in test.iterrows(): 
    
    # return indices of nearest neighbors with complete features
    ind = knn.kneighbors(X = row[impute_cols].values.reshape(1,-1), return_distance=False)[0] 

    # for each feature that're NaNs    
    for col in test.columns.values[row.isnull().values]: 
        # get vals from nearest neighbors for this col
        train_vals = np.array(train.loc[ind, col]) 
        
        # if indicator 
        if train[col].unique().shape[0] <= 2: 
            # fill w/ mode
            test.loc[i, col] = stats.mode(train_vals)[0][0] 
        else:
            # fill w/ mean
            test.loc[i, col] = train_vals.mean() 
{% endhighlight %}

The resulting dataset is available in [`data_ready.csv`](https://www.dropbox.com/s/e2hwpdninexi7cc/data_ready.csv?dl=0) 
on Dropbox.

This approach is imperfect but also intuitive and advantageous from a complexity standpoint. 
It is available in [`III_ChicagoFoodInspection_CS109a_Imputation_and_Encoding.ipynb`](https://github.com/Fggw/foodinspections/blob/master/III_ChicagoFoodInspection_CS109a_Imputation_and_Encoding.ipynb).