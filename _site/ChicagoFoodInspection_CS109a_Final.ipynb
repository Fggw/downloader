{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pull, parsing and preliminary modeling\n",
    "#### CS109a - Sam, Luke, Jake, Jeremy\n",
    "#### Chicago Food Inspection Forecasting\n",
    "#### Last Updated: 11/28/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from download import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import optparse\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sys import exit\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main(date=datetime.datetime.strptime('01/01/2010', '%m/%d/%Y'), export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the data:\n",
    "\n",
    "**NOTE: SWAP result binary later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import optparse\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sys import exit\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('/Users/jeremywelborn1/Documents/Jeremy/Harvard/Classes/III_Junior/1st_CS109a/DOWNLOADED_DATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_possibilities = ['Pass', 'Fail', 'Pass w/ Conditions']\n",
    "valid_result_bools = [result in result_possibilities for result in data.results]\n",
    "data = data[valid_result_bools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fail(result):\n",
    "    if 'Fail' in result:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def grocery_store(facility_type):\n",
    "    if 'Grocery Store' == facility_type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['result_binary'] = [fail(result) for result in data.results]\n",
    "data['grocery'] = [grocery_store(facility_type) for facility_type in data.facility_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_inspection_types = data.groupby('inspection_type').count().result_binary.sort_values(ascending=False).index[:10]\n",
    "top_inspection_types_sorted = data.groupby('inspection_type').mean().result_binary[top_inspection_types].sort_values(ascending=False).index\n",
    "\n",
    "fig = plt.figure() # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "width = 0.4\n",
    "\n",
    "data.groupby('inspection_type').mean().result_binary[top_inspection_types].sort_values(ascending=False).plot(kind='bar',color='r',ax = ax, alpha=0.7,width=width,position=0)\n",
    "data.groupby('inspection_type').count().result_binary[top_inspection_types_sorted].plot(kind='bar',color='b',ax = ax2, alpha=0.7,width=width,position=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inspection_type_dummies = pd.get_dummies(data['inspection_type'])\n",
    "inspection_type_dummies = inspection_type_dummies[top_inspection_types]\n",
    "\n",
    "\n",
    "data = pd.concat([data,inspection_type_dummies],axis=1,join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "biz_cols = []\n",
    "for col in data.columns:\n",
    "    if 'business_activity' in col:\n",
    "        biz_cols.append(col)\n",
    "        \n",
    "count = []\n",
    "mean = []\n",
    "for col in biz_cols:\n",
    "    count.append(data[data[col] == 1].result_binary.mean())\n",
    "    mean.append(len(data[data[col] == 1].result_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biz_cols_mean = pd.Series(mean,biz_cols)\n",
    "biz_cols_count = pd.Series(count,biz_cols)\n",
    "\n",
    "biz_cols_count_top = biz_cols_count.sort_values(ascending=False).index[:10]\n",
    "\n",
    "\n",
    "biz_col_mean_sorted = biz_cols_mean[biz_cols_count_top].sort_values(ascending=False).index\n",
    "\n",
    "fig = plt.figure() # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "width = 0.4\n",
    "\n",
    "biz_cols_mean[biz_col_mean_sorted].sort_values(ascending=False).plot(kind='bar',color='r',ax = ax, alpha=0.7,width=width,position=0)\n",
    "biz_cols_count[biz_col_mean_sorted].plot(kind='bar',color='b',ax = ax2, alpha=0.7,width=width,position=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "biz_cols_counts = data[biz_cols].sum().sort_values()\n",
    "\n",
    "\n",
    "pass_rate = []\n",
    "count = []\n",
    "for col in biz_cols:\n",
    "    pass_rate.append(data[data[col] == 1].result_binary.mean())\n",
    "    pass_rate.append(data[data[col] == 1].result_binary.mean())\n",
    "    \n",
    "    \n",
    "top_n = 20\n",
    "top_license_counts = biz_cols_counts.ix[biz_cols_counts.index[-top_n:]]\n",
    "\n",
    "top_biz_counts_cols = list(biz_cols_counts.index[-top_n:])\n",
    "\n",
    "pass_rate = []\n",
    "for col in top_biz_counts_cols:\n",
    "    pass_rate.append(data[data[col] == 1].result_binary.mean())\n",
    "    \n",
    "top_biz_pass_rate = pd.Series(data = np.array(pass_rate),index=top_biz_counts_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(1-top_biz_pass_rate).plot(kind='bar')\n",
    "plt.show()\n",
    "pd.DataFrame(top_license_counts).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_n=5\n",
    "all_license_cols = list(biz_cols)\n",
    "for col in list(biz_cols_counts.index[-keep_n:]):\n",
    "    all_license_cols.remove(col)\n",
    "\n",
    "drop_license_cols = all_license_cols\n",
    "data = data.drop(drop_license_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['inspection_date'] = pd.to_datetime(data['inspection_date'])\n",
    "#data['license_start_date'] = pd.to_datetime(data['license_start_date'])\n",
    "#data['date_issued'] = pd.to_datetime(data['date_issued'])\n",
    "data['creation_date'] = pd.to_datetime(data['creation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weather_data_ohare = pd.read_csv('Ohare Weather Data.csv')\n",
    "# weather_data_midway = pd.read_csv('Midway Weather Data.csv')\n",
    "weather_data_ohare = pd.read_csv('/Users/jeremywelborn1/Documents/Jeremy/Harvard/Classes/III_Junior/1st_CS109a/Ohare Weather Data.csv')\n",
    "weather_data_midway = pd.read_csv('/Users/jeremywelborn1/Documents/Jeremy/Harvard/Classes/III_Junior/1st_CS109a/Midway Weather Data.csv')\n",
    "\n",
    "weather_data_ohare['inspection_date'] = [datetime.datetime.strptime(str(DATE), '%Y%m%d') for DATE in weather_data_ohare.DATE]\n",
    "weather_data_midway['inspection_date'] = [datetime.datetime.strptime(str(DATE), '%Y%m%d') for DATE in weather_data_midway.DATE]\n",
    "temp_data_ohare = weather_data_ohare.set_index(['inspection_date'])[['TMAX']]\n",
    "temp_data_midway = weather_data_midway.set_index(['inspection_date'])[['TMAX']]\n",
    "\n",
    "temp_data_ohare['TMAX_3DayAvg'] = pd.rolling_mean(temp_data_ohare.TMAX,3)\n",
    "temp_data_ohare['TMAX_10DayAvg'] = pd.rolling_mean(temp_data_ohare.TMAX,10)\n",
    "temp_data_ohare['TMAX_30DayAvg'] = pd.rolling_mean(temp_data_ohare.TMAX,30)\n",
    "\n",
    "temp_data_midway['TMAX_3DayAvg'] = pd.rolling_mean(temp_data_midway.TMAX,3)\n",
    "temp_data_midway['TMAX_10DayAvg'] = pd.rolling_mean(temp_data_midway.TMAX,10)\n",
    "temp_data_midway['TMAX_30DayAvg'] = pd.rolling_mean(temp_data_midway.TMAX,30)\n",
    "data = data.join(temp_data_midway, on='inspection_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "critical_flags = re.compile(r'(\\bCRITICAL\\b)',flags = re.IGNORECASE)\n",
    "serious_flags = re.compile(r'(\\bSERIOUS\\b)',flags = re.IGNORECASE)\n",
    "citation_flags = re.compile(r'(\\bCITATION\\b)',flags = re.IGNORECASE)\n",
    "violation_flags = re.compile(r'(\\bVIOLATION\\b)',flags = re.IGNORECASE)\n",
    "minor_flags = re.compile(r'(\\bMINOR\\b)',flags = re.IGNORECASE)\n",
    "corrected_flags = re.compile(r'(\\bCORRECTED\\b)',flags = re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "violations_count = []\n",
    "citations_count = []\n",
    "critical_count = []\n",
    "serious_count = []\n",
    "minor_count = []\n",
    "corrected_count = []\n",
    "\n",
    "LOG_EVERY_N = 10000\n",
    "\n",
    "\n",
    "  \n",
    "for index in data.index:\n",
    "    text = data.ix[index].violations\n",
    "    \n",
    "    if type(text) == type(' '):\n",
    "        violations_count.append(violation_flags.subn('',text)[1])\n",
    "        citations_count.append(citation_flags.subn('',text)[1])\n",
    "        critical_count.append(critical_flags.subn('',text)[1])\n",
    "        serious_count.append(serious_flags.subn('',text)[1])\n",
    "        minor_count.append(minor_flags.subn('',text)[1])\n",
    "        corrected_count.append(corrected_flags.subn('',text)[1])\n",
    "\n",
    "\n",
    "    else:\n",
    "        violations_count.append(float('nan'))\n",
    "        citations_count.append(float('nan'))\n",
    "        critical_count.append(float('nan'))\n",
    "        serious_count.append(float('nan'))\n",
    "        minor_count.append(float('nan'))\n",
    "        corrected_count.append(float('nan'))\n",
    "\n",
    "        \n",
    "    if (index % LOG_EVERY_N) == 0:\n",
    "        print index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['violations_count'] = violations_count\n",
    "data['citations_count'] = citations_count\n",
    "data['critical_count'] = critical_count\n",
    "data['serious_count'] = serious_count\n",
    "data['minor_count'] = minor_count\n",
    "data['corrected_count'] = corrected_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare pass rates for different violation counts\n",
    "bins = np.linspace(0,6,7)\n",
    "data[data.result_binary == 1].citations_count.hist(normed=False,alpha=0.5,bins=bins)\n",
    "data[data.result_binary == 0].citations_count.hist(normed=False,alpha=0.5,bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prev_inspection_features(data,inspection_id,date):\n",
    "    current_inspection = data.ix[data[data.inspection_id==inspection_id].index[0]]\n",
    "    \n",
    "    all_inspections = data[data.license_ == current_inspection.license_]\n",
    "    \n",
    "    previous_inspections = all_inspections[all_inspections.inspection_date < date]\n",
    "    if len(previous_inspections) >= 1:\n",
    "        last_inspection_index = previous_inspections.inspection_date.sort_values().index[-1]\n",
    "        fraction = previous_inspections.result_binary.mean()\n",
    "        count = previous_inspections.result_binary.count()\n",
    "        last = previous_inspections.result_binary[last_inspection_index]\n",
    "        \n",
    "        #if type(pd.to_datetime('nan')) != type(current_inspection.license_start_date):\n",
    "        #    license_age = current_inspection.inspection_date - current_inspection.license_start_date\n",
    "        #    license_age = license_age.days\n",
    "        #    if license_age <= 0:\n",
    "        #        license_age = 0\n",
    "        #else:\n",
    "        #    license_age = float('nan')\n",
    "            \n",
    "        time_since_last = current_inspection.inspection_date - previous_inspections.ix[last_inspection_index].inspection_date\n",
    "        time_since_last = time_since_last.days\n",
    "        \n",
    "        prev_violations = previous_inspections.violations_count[last_inspection_index]\n",
    "        prev_citations = previous_inspections.citations_count[last_inspection_index]\n",
    "        prev_critical = previous_inspections.critical_count[last_inspection_index]\n",
    "        prev_serious = previous_inspections.serious_count[last_inspection_index]\n",
    "        prev_minor = previous_inspections.minor_count[last_inspection_index]\n",
    "        prev_corrected = previous_inspections.corrected_count[last_inspection_index]\n",
    "\n",
    "    else:\n",
    "        fraction = float('nan')\n",
    "        count = 0\n",
    "        last = float('nan')\n",
    "        license_age = float('nan')\n",
    "        time_since_last = float('nan')\n",
    "        prev_violations = float('nan')\n",
    "        prev_citations = float('nan')\n",
    "        prev_critical = float('nan')\n",
    "        prev_serious = float('nan')\n",
    "        prev_minor = float('nan')\n",
    "        prev_corrected = float('nan')\n",
    "\n",
    "    return np.array([count,\n",
    "                     fraction,\n",
    "                     last,\n",
    "                     #license_age,\n",
    "                     time_since_last,\n",
    "                     prev_violations,\n",
    "                     prev_citations,\n",
    "                     prev_critical,\n",
    "                     prev_serious,\n",
    "                     prev_minor,\n",
    "                     prev_corrected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "previous_inspection_features = np.zeros((len(data.index),10))\n",
    "\n",
    "LOG_EVERY_N = 10000\n",
    "for i in range(len(data.index)):\n",
    "    index = data.index[i]\n",
    "    previous_inspection_features[i,:] = prev_inspection_features(data,data.ix[index].inspection_id,data.ix[index].inspection_date)\n",
    "    if (index % LOG_EVERY_N) == 0:\n",
    "        print index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['previous_count'] = previous_inspection_features[:,0]\n",
    "data['previous_fraction'] = previous_inspection_features[:,1]\n",
    "data['previous_result'] = previous_inspection_features[:,2]\n",
    "#data['license_age'] = previous_inspection_features[:,3]\n",
    "data['time_since_last_inspection'] = previous_inspection_features[:,3]\n",
    "data['previous_violations'] = previous_inspection_features[:,4]\n",
    "data['previous_citations'] = previous_inspection_features[:,5]\n",
    "data['previous_critical'] = previous_inspection_features[:,6]\n",
    "data['previous_serious'] = previous_inspection_features[:,7]\n",
    "data['previous_minor'] = previous_inspection_features[:,8]\n",
    "data['previous_corrected'] = previous_inspection_features[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('data_built_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('BACKUP.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#adjust number of buckets using np.linspace ----------------->\n",
    "yedges = np.linspace(data.longitude.min(),data.longitude.max(),6)\n",
    "xedges = np.linspace(data.latitude.min(),data.latitude.max(),6)\n",
    "\n",
    "x = np.array(data.latitude)\n",
    "y = np.array(data.longitude)\n",
    "\n",
    "x1 = np.array(data[data.result_binary == 1].latitude)\n",
    "y1 = np.array(data[data.result_binary == 1].longitude)\n",
    "\n",
    "x0 = np.array(data[data.result_binary == 0].latitude)\n",
    "y0 = np.array(data[data.result_binary == 0].longitude)\n",
    "\n",
    "\n",
    "hist0, xedges, yedges = np.histogram2d(x0, y0, (xedges, yedges))\n",
    "hist1, xedges, yedges = np.histogram2d(x1, y1, (xedges, yedges))\n",
    "\n",
    "hist = hist0/(hist1+hist0)\n",
    "hist_count = hist1+hist0\n",
    "\n",
    "xidx = np.clip(np.digitize(x, xedges), 0, hist.shape[0]-1)\n",
    "yidx = np.clip(np.digitize(y, yedges), 0, hist.shape[1]-1)\n",
    "\n",
    "c = hist[xidx, yidx]\n",
    "plt.scatter(x, y, c=c)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sorted_index = np.argsort(hist.reshape(1,-1))\n",
    "\n",
    "print hist_count.reshape(1,-1)[0][sorted_index[0]]\n",
    "print hist.reshape(1,-1)[0][sorted_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['bin_index'] = hist.shape[1]*yidx + xidx\n",
    "\n",
    "fig = plt.figure(figsize=(10,10)) # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "\n",
    "data.groupby('bin_index').count().result_binary.plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "data.groupby('bin_index').mean().result_binary.plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_index = data.groupby('license_description').mean().result_binary.sort_values().index\n",
    "data.groupby('license_description').mean().result_binary.sort_values()[result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(10,10)) # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "\n",
    "data.groupby('license_description').count().result_binary[result_index].plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "data.groupby('license_description').mean().result_binary.sort_values()[result_index].plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_50 = data.groupby('facility_type').count().result_binary.sort_values().index[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.groupby('facility_type').mean().result_binary[top_50].sort_values()\n",
    "\n",
    "top_50count_mean_sorted_index = data.groupby('facility_type').mean().result_binary[top_50].sort_values().index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(10,10)) # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "\n",
    "data.groupby('facility_type').mean().result_binary[top_50].sort_values().plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "data.groupby('facility_type').count().result_binary[top_50count_mean_sorted_index].plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "#### All shit above is Luke's work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('../data_built_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in data.columns.values:\n",
    "    print col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we should return to this and check which covariates we've selected \n",
    "\n",
    "cols = ['inspection_date',\n",
    "        'facility_type',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'results',\n",
    "        'risk',\n",
    "        'business_activity_Consumption of Liquor on Premises',\n",
    "        'business_activity_Preparation of Food and Dining on Premise With Seating',\n",
    "        'business_activity_Retail Sale of Tobacco',\n",
    "        'business_activity_Retail Sales of Packaged Liquor',\n",
    "        'business_activity_Retail Sales of Perishable Foods',\n",
    "        'Canvass', # inspection_type\n",
    "        'License',\n",
    "        'Canvass Re-Inspection',\n",
    "        'Complaint',\n",
    "        'License Re-Inspection',\n",
    "        'Short Form Complaint',\n",
    "        'Complaint Re-Inspection',\n",
    "        'Suspected Food Poisoning',\n",
    "        'Consultation',\n",
    "        'License-Task Force',\n",
    "        'point_crime_count',\n",
    "        'point_sanit_count',\n",
    "        'result_binary',\n",
    "        'TMAX',\n",
    "        'TMAX_3DayAvg',\n",
    "        'TMAX_10DayAvg',\n",
    "        'TMAX_30DayAvg',\n",
    "        'previous_count',\n",
    "        'previous_fraction',\n",
    "        'previous_result',\n",
    "        'time_since_last_inspection',\n",
    "        'previous_violations',\n",
    "        'previous_citations',\n",
    "        'previous_critical',\n",
    "        'previous_serious',\n",
    "        'previous_minor',\n",
    "        'previous_corrected']\n",
    "\n",
    "print data[cols].shape\n",
    "print data.shape\n",
    "\n",
    "# select specific features\n",
    "data_sub = data[cols]\n",
    "# what would happen if we dropped NaNs\n",
    "print data_sub.dropna(axis=0, how='any').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub = data_sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temporary -- dumping correct crime data and sanitation complaint data into dataset\n",
    "data_dump = pd.read_csv('../data_with_crime_sanit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sub['crime'] = data_dump['crime'].copy()\n",
    "data_sub['sanit'] = data_dump['sanitation'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert string to datetime to be safe\n",
    "data_sub['inspection_date'] = pd.to_datetime(data_sub['inspection_date'])\n",
    "\n",
    "print data_sub['inspection_date'].min()\n",
    "print data_sub['inspection_date'].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation and encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# non-KNN approach:\n",
    "# replace null values with feature mean (quantitative) or feature mode (qualitative)\n",
    "# for column in data_sub.columns:\n",
    "#     if (data_sub[column].unique().shape[0] < 8) or (data_sub[column].dtype == np.dtype('object')):\n",
    "#         data_sub[column].fillna(value = data_sub[column].mode()[0], inplace = True)\n",
    "#     else:\n",
    "#         data_sub[column].fillna(value = data_sub[column].mean(), inplace = True)\n",
    "\n",
    "# check no null values\n",
    "nan_cnt = 0\n",
    "for i, column in enumerate(data_sub.columns):\n",
    "    print i, column, np.array(data_sub[column].isnull()).sum()\n",
    "    nan_cnt = nan_cnt + np.array(data_sub[column].isnull()).sum()\n",
    "    \n",
    "print 'total cells:', data_sub.size\n",
    "print 'nan_cnt:', nan_cnt\n",
    "print 'ratio for sam:', nan_cnt / float(data_sub.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how should we handle the historical count data?\n",
    "# there's no optimal solution: either (1) impute, biasing that they wouldn't have done well \n",
    "# or (2) set to 0, biasing that they would've done well? We'll set to 0 for simplicity.\n",
    "\n",
    "print 'count of obs for which previous_count == 0:', data_sub[data_sub['previous_count'] == 0.0].shape[0]\n",
    "print 'this corresponds with number of nans for previous_fraction, previous_result, time_since_last_inspection, which is what we\\'d expect'\n",
    "\n",
    "# set all these to 0 then\n",
    "hist_cols = ['previous_fraction',\n",
    "            'previous_result',\n",
    "            'time_since_last_inspection',\n",
    "            'previous_violations',\n",
    "            'previous_citations',\n",
    "            'previous_critical',\n",
    "            'previous_serious',\n",
    "            'previous_minor',\n",
    "            'previous_corrected']\n",
    "\n",
    "for hist_col in hist_cols:\n",
    "    data_sub.loc[data_sub['previous_count'] == 0.0, hist_col] = data_sub.loc[data_sub['previous_count'] == 0.0, hist_col].fillna(value = 0)\n",
    "    \n",
    "print '\\n here are NaN counts after this step: \\n'\n",
    "for i, column in enumerate(data_sub.columns):\n",
    "    print i, column, np.array(data_sub[column].isnull()).sum()\n",
    "    \n",
    "print '37-42 still have a uniform number of null values because they had no text from the inspection, so nothing to scrape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obs with no null values\n",
    "print 'count of obs with no null values:', data_sub.shape[0] - data_sub.isnull().any(axis = 1).sum()\n",
    "print 'count of features with no null values:', data_sub.shape[1] - data_sub.isnull().any(axis = 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# features with no null values we can use to impute\n",
    "data_sub.isnull().any(axis = 0)[data_sub.isnull().any(axis = 0) == False].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so that the imputer has enough predictors, to work with, fill in risk by mean / mode approach since only 63 obs have nulls\n",
    "\n",
    "data_sub['risk'].fillna(value = data_sub['risk'].mode()[0], inplace = True)\n",
    "\n",
    "# same\n",
    "# data_sub['inspection_type'].fillna(value = data_sub['inspection_type'].mode()[0], inplace = True)\n",
    "\n",
    "# check\n",
    "print 'number of null values in risk:', data_sub['risk'].isnull().sum()\n",
    "# print 'number of null values in inspection_type:', data_sub['inspection_type'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN \n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have to encode the data first... but facility_type is annoying to do...\n",
    "print data_sub.facility_type.value_counts()[0:10]\n",
    "\n",
    "take_types = data_sub.facility_type.value_counts()[0:50].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# why don't we just take the first fifty for now \n",
    "# this takes awhile... a faster approach?\n",
    "#\n",
    "# READ THIS IN, DONT BOTHER RUNNING AGAIN\n",
    "#\n",
    "for i, row in data_sub[['facility_type']].iterrows():\n",
    "    if row.values not in take_types:\n",
    "        data_sub.loc[i, 'facility_type'] = 'Other'\n",
    "    else:\n",
    "        continue\n",
    "    if (i % 500 == 0):\n",
    "        print 'finished iteration:', i\n",
    "        \n",
    "        \n",
    "# it's encoded in this, so just kidding\n",
    "# hack_data = data = pd.read_csv('../data_ready.csv')\n",
    "# hack_data['facility_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check response\n",
    "print 'Useless response count:', sum((data_sub.results != 'Pass') & (data_sub.results != 'Pass w/ Conditions') & (data_sub.results != 'Fail'))\n",
    "indices = (data_sub.results == 'Pass') | (data_sub.results == 'Pass w/ Conditions') | (data_sub.results == 'Fail')\n",
    "data_sub = data_sub[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# risk\n",
    "indices = (data_sub.risk == 'Risk 1 (High)') | (data_sub.risk == 'Risk 2 (Medium)') | (data_sub.risk == 'Risk 3 (Low)')\n",
    "data_sub = data_sub[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub = data_sub.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have to these encode first\n",
    "data_sub_encoded = pd.DataFrame({})\n",
    "for column in data_sub.columns:\n",
    "#     if (data_sub[column].unique().shape[0] < 8) or data_sub[column].dtype == np.dtype('object'):\n",
    "    if (data_sub[column].dtype == np.dtype('object')) & (column not in ['results']):\n",
    "        encoding = pd.get_dummies(data_sub[column])\n",
    "        data_sub_encoded = pd.concat([data_sub_encoded, encoding], axis = 1)\n",
    "    else:\n",
    "#         data_sub_encoded = pd.concat([data_sub_encoded, data_sub[[column]].astype(float)], axis = 1)\n",
    "        data_sub_encoded = pd.concat([data_sub_encoded, data_sub[[column]]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train data are observations that are complete\n",
    "# test data (which we wish to impute) are observations that are incomplete\n",
    "train = data_sub_encoded[np.logical_not(data_sub_encoded.isnull().any(axis = 1).values)]\n",
    "test = data_sub_encoded[data_sub_encoded.isnull().any(axis = 1).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset indices because we've dropped rows, NearestNeighbors was returning wrong indices because they weren't reset\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dont actually have to split x and y since we specify what features we're imputing on in impute_cols\n",
    "# x_train = train.drop(['result_binary', 'results'], axis = 1) \n",
    "# x_test = test.drop(['result_binary', 'results'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sub.isnull().any(axis = 0)[data_sub.isnull().any(axis = 0) == False].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should be using 'inspection_type', facility_type' too but i will return to this, was frustrated with the encoding!\n",
    "# these have to be encoded to work with knn, but when encoded it is hard to get them...\n",
    "\n",
    "impute_cols = ['Risk 1 (High)', 'Risk 2 (Medium)', 'Risk 3 (Low)', 'previous_count', 'previous_fraction',\n",
    "       'previous_result', 'time_since_last_inspection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors = 5)\n",
    "knn.fit(train[impute_cols]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return k nearest neighbors based on features we have for all observations\n",
    "# then fill in other values using these nearest neighbors \n",
    "\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "for i, row in test.iterrows(): # for each observation with incomplete features\n",
    "    ind = knn.kneighbors(X = row[impute_cols].values.reshape(1,-1), return_distance=False)[0] # return indices of nearest neighbors with complete features\n",
    "    for col in test.columns.values[row.isnull().values]: # for each feature that're NaNs    \n",
    "        train_vals = np.array(train.loc[ind, col]) # get vals from nearest neighbors for this col\n",
    "        if train[col].unique().shape[0] <= 2: # if indicator \n",
    "            test.loc[i, col] = stats.mode(train_vals)[0][0] # fill w/ mode\n",
    "        else:\n",
    "            test.loc[i, col] = train_vals.mean() # fill w/ mean\n",
    "    if (i % 500 == 0):\n",
    "        print 'finished iteration:', i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this won't work, apply flattens the thing\n",
    "# import math\n",
    "# from scipy import stats\n",
    "\n",
    "# def impute_row(row):\n",
    "#     ind = knn.kneighbors(X = row[impute_cols].values.reshape(1, -1), return_distance=False)[0]\n",
    "#     for col in test.columns.values[row.isnull().values]: # for each feature that're NaNs    \n",
    "#         train_vals = np.array(train.loc[ind, col])\n",
    "#         if train[col].unique().shape[0] <= 2:\n",
    "#             row[col] = stats.mode(train_vals)[0][0]\n",
    "#         else:\n",
    "#             row[col] = train_vals.mean()\n",
    "\n",
    "# test = test.apply(impute_row, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ready = pd.concat([train, test], axis = 0)\n",
    "data_ready = data_ready.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reorder = ['inspection_date','result_binary','results','point_crime_count', 'point_sanit_count',\n",
    "#       'latitude', 'longitude','TMAX','TMAX_3DayAvg', 'TMAX_10DayAvg', 'TMAX_30DayAvg',\n",
    "#      'Risk 1 (High)', 'Risk 2 (Medium)', 'Risk 3 (Low)',\n",
    "#         'previous_count','previous_fraction', 'previous_result',\n",
    "#        'time_since_last_inspection', 'previous_violations',\n",
    "#        'previous_citations', 'previous_critical', 'previous_serious',\n",
    "#        'previous_minor', 'previous_corrected','inspection_type',\n",
    "#         \"1023 CHILDERN'S SERVICES FACILITY\",\n",
    "#        \"1023-CHILDREN'S SERVICES FACILITY\", 'ASSISTED LIVING', 'BANQUET',\n",
    "#        'BANQUET HALL', 'Bakery', 'CAFETERIA', 'CHURCH', 'CHURCH KITCHEN',\n",
    "#        'CONVENIENCE', 'CONVENIENCE STORE', 'CULINARY SCHOOL', 'Catering',\n",
    "#        \"Children's Services Facility\", 'DAYCARE', 'Daycare (2 - 6 Years)',\n",
    "#        'Daycare (2 Years)', 'Daycare (Under 2 Years)',\n",
    "#        'Daycare Above and Under 2 Years', 'Daycare Combo 1586',\n",
    "#        'GAS STATION', 'GAS STATION/MINI MART', 'GROCERY/RESTAURANT',\n",
    "#        'Golden Diner', 'Grocery Store', 'Hospital', 'KIOSK',\n",
    "#        'LIVE POULTRY', 'Liquor', 'Long Term Care', 'Mobile Food Dispenser',\n",
    "#        'Mobile Food Preparer', 'Navy Pier Kiosk', 'Other',\n",
    "#        'PRIVATE SCHOOL', 'RESTAURANT/BAR', 'RESTAURANT/GROCERY STORE',\n",
    "#        'ROOF TOPS', 'ROOFTOP', 'Restaurant', 'STADIUM', 'STORE', 'School',\n",
    "#        'Shared Kitchen', 'Shared Kitchen User (Long Term)',\n",
    "#        'Shared Kitchen User (Short Term)', 'Shelter', 'Special Event',\n",
    "#        'TAVERN', 'Wholesale', 'convenience store', \n",
    "#        'business_activity_Catering of Liquor To Events',\n",
    "#        'business_activity_Consumption of Liquor on Premises',\n",
    "#        'business_activity_Hotel - 7 or More Sleeping Rooms',\n",
    "#        'business_activity_Operation of a Fuel Filling Station',\n",
    "#        'business_activity_Preparation of Food and Dining on Premise With Seating',\n",
    "#        'business_activity_Provides Onsite Amusement or Entertainment',\n",
    "#        'business_activity_Retail Sale of Tobacco',\n",
    "#        'business_activity_Retail Sales of Packaged Liquor',\n",
    "#        'business_activity_Retail Sales of Packaged Liquor on Sundays from 8AM - 11AM | Retail Sales of Packaged Liquor',\n",
    "#        'business_activity_Retail Sales of Packaged Liquor | Retail Sales of Packaged Liquor on Sundays from 8AM - 11AM',\n",
    "#        'business_activity_Retail Sales of Perishable Foods',\n",
    "#        'business_activity_Retail Sales of Tobacco Products',\n",
    "#        'business_activity_Sale of Food Prepared Onsite With Dining Area',\n",
    "#        'business_activity_Sale of Food Prepared Onsite Without Dining Area',\n",
    "#        'business_activity_Sale of Liquor Outdoors on Private Property',\n",
    "#        'business_activity_Sale of Liquor Until 4am, Monday - Saturday and 5am on Sunday',\n",
    "#        'business_activity_Supervision of, and Care for, Children 0-6 Years of Age, During the Day Between 6am-9pm',\n",
    "#        'business_activity_Supervision of, and Care for, Children 2-6 Years of Age, During the Day Between 6am-9pm',\n",
    "#        'business_activity_Supervision of, and Care for, Children Under 2 Years of Age, During the Day Between 6am-9pm | Supervision of, and Care for, Children 2-6 Years of Age, During the Day Between 6am-9pm',\n",
    "#        'business_activity_Tavern - Consumption of Liquor on Premise']\n",
    "\n",
    "# data_ready = data_ready[reorder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so don't have to rerun the code above\n",
    "data_ready.to_csv('/Users/jeremywelborn1/Documents/Jeremy/Harvard/Classes/III_Junior/1st_CS109a/fggw/data_ready.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring function \n",
    "**How we select a model and how this reflects the model put into practice:**  \n",
    "Ultimately, we care only that we correctly identify failed inspections among the inspections that the City of Chicago actually has the resources to carry out. So, we'd like our model to produce probabilities of passing or not passing -- the inspectors can then inspect establishments with highest probailities of not passing. We'd also like to punish or penalize the model for sending inspectors to inspect establishments that pass but that we predict not to pass with high certainty (or vice versa) (i.e. we'd like to be less wrong when we're not right). \n",
    "\n",
    "The log loss is a suitable objective function to optimize then. The log loss is the negative log likelihood of a Bernoulli random variable (in the 2-class setting, we'll justify this shortly): $$-\\frac{1}{n}\\sum_1^n y_i \\log(p_i)-(1-y_i)\\log(1-p_i)$$ for $n$ observations, where the $i$th observation is of correct class $y_i \\in \\{0,1\\}$ which our model predicts with probability $p_i$. This achieves specifically this sort of penality (suppose $y_i=1$ and we predict this with probability of only $0.1$, this yields a value that approaches $-\\infty$ rapidly).\n",
    "\n",
    "When put into practice, this approach ranks inspections by probability of not passing, so inspectors can carry out inspections that appear at the top of this ranking. Selecting a model with minimal log loss (or maximum likelihood) essentially ensures this ranking is best, that inspectors have the highest chance of inspecting establishments that have commited a violation. Ultimately, in a given time frame with constrained resources, the City of Chicago cannot carry out all the inspections it has to do. There is a cutoff, so they will carry out the inspections at the top of this ranking. \n",
    "\n",
    "How can we get confidence in this approach (what does it look like in practice)? Suppose this model is run only one time (that's the best we can do, we don't have anymore data) and we use the resulting ranking to allocate our inspectors to inspections for $n$ inspections (we take $n$ to be the number of inspections the inspectors were able to do in, say, a month). We then compare the number of failures correctly classified in the top $n$ inspections in the ranking (since there are what would've been done using our best model and methodology) to the number of failures actually found in this same timeframe. If it is greater, then we should be confident in how the model is allocating the City of Chicago's resources. (N.B. of course, the same number of failures will all be discovered over time, but we care about early intervention from a public health perspective!).\n",
    "\n",
    "Also note that a 2-class approach is sufficient here - we care about failing (vs. not failing). 'Pass' vs. 'Pass w/ Conditions' is an unimportant distinction for how this model is motivated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# true is list of true classes, pred is list of predicted class probabilities \n",
    "def score(y, p_hat):\n",
    "    p_hat = map(lambda t: t[1], p_hat) # p_hat returned from sklearn is a list of lists with p for both classes, we want p for class 1 or fail \n",
    "    epsilon = 1e-15\n",
    "    p_hat = sp.maximum(epsilon, p_hat)\n",
    "    p_hat = sp.minimum(1-epsilon, p_hat)\n",
    "    logloss = sum(y*sp.log(p_hat) + sp.subtract(1,y)*sp.log(sp.subtract(1,p_hat)))\n",
    "    logloss = logloss * -1.0/len(y)\n",
    "    return logloss\n",
    "\n",
    "# def score(act, pred, n):\n",
    "#     act = act[:n]\n",
    "#     pred = pred(:n)\n",
    "#     epsilon = 1e-15\n",
    "#     pred = sp.maximum(epsilon, pred)\n",
    "#     pred = sp.minimum(1-epsilon, pred)\n",
    "#     ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "#     ll = ll * -1.0/len(act)\n",
    "#     return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make models\n",
    "Logistic\n",
    "Poly logistic\n",
    "LDA\n",
    "QDA\n",
    "k-NN\n",
    "Trees (bagging RF, boosting AdaBoost)\n",
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import optparse\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sys import exit\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest \n",
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoost\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/jeremywelborn1/Documents/Jeremy/Harvard/Classes/III_Junior/1st_CS109a/fggw/data_ready.csv')\n",
    "# dataset = data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to access the year conveniently\n",
    "dataset['inspection_date'] = pd.to_datetime(dataset['inspection_date'])\n",
    "dataset['inspection_date'] = [day.date() for day in dataset['inspection_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-04\n",
      "2016-12-02\n"
     ]
    }
   ],
   "source": [
    "print dataset['inspection_date'].min()\n",
    "print dataset['inspection_date'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONT RUN THIS, Luke adjusted 0, 1 labeling up above\n",
    "# swap 0s and 1s to make neg log loss scoring functions more easy to interpret (we care about fails)\n",
    "# dataset.replace(to_replace = {'result_binary': {0: 'not pass', 1: 'pass'}}, inplace = True)\n",
    "# dataset.replace(to_replace = {'result_binary': {'not pass': 1, 'pass': 0}}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confirm this \n",
    "# dataset.loc[0:10, ['result_binary', 'results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2010, 2011, 2012, 2013, 2014, 2015, 2016])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(map(lambda d: d.year, dataset['inspection_date'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# withold 2010 for lookback period to build previous features\n",
    "# train on 2011-2015\n",
    "train = dataset[map(lambda d: (d.year != 2010) & (d.year != 2016), dataset['inspection_date'].values)] \n",
    "# test on 2016 - not this notebook, this is Luke\n",
    "test = dataset[map(lambda d: d.year == 2016, dataset['inspection_date'].values)] \n",
    "\n",
    "x_train = train.drop(['result_binary', 'results', 'inspection_date'], axis = 1) # do we want inspection_date anymore? annoying with sklearn functions...\n",
    "y_train = train['result_binary']\n",
    "x_test = test.drop(['result_binary', 'results', 'inspection_date'], axis = 1) \n",
    "y_test = test['result_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120304, 93) (87180, 93) (15742, 93)\n",
      "n.b.: not including observations from 2010\n"
     ]
    }
   ],
   "source": [
    "print dataset.shape, train.shape, test.shape\n",
    "print 'n.b.: not including observations from 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count pass: 68510\n",
      "Count not pass: 18670\n"
     ]
    }
   ],
   "source": [
    "# correct class imbalance in the train data\n",
    "print 'Count pass:', sum(y_train == 0)\n",
    "print 'Count not pass:', sum(y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18646092,  2.19330602,  0.19956804, ...,  0.22694735,\n",
       "        0.22694735, -0.28743414])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.scale(x_train['crime'], copy = False)\n",
    "preprocessing.scale(x_train['sanit'], copy = False)\n",
    "preprocessing.scale(x_test['crime'], copy = False)\n",
    "preprocessing.scale(x_test['sanit'], copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## subsampling algorithm \n",
    "# takes x_train, y_train and returns a class-balanced x_train, y_train \n",
    "# assumes count of class 0 > count of class 1\n",
    "# subsampled down so a bunch of data disappears - we can change this if we want \n",
    "def subsample(x_train, y_train):\n",
    "    train = pd.concat([x_train, y_train], axis = 1)\n",
    "\n",
    "    train_0 = train[y_train == 0]\n",
    "    train_1 = train[y_train == 1]\n",
    "    \n",
    "    train_0_subsample = train_0.sample(train_1.shape[0])\n",
    "    \n",
    "    train_subsample = pd.concat([train_0_subsample, train_1], axis = 0)\n",
    "    \n",
    "    x_train_subsample = train_subsample.iloc[:, :-1]\n",
    "    y_train_subsample = train_subsample.iloc[:, -1]\n",
    "    \n",
    "    return x_train_subsample, y_train_subsample\n",
    "    \n",
    "x_train_sub, y_train_sub = subsample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count pass: 18670\n",
      "Count not pass: 18670\n"
     ]
    }
   ],
   "source": [
    "print 'Count pass:', sum(y_train_sub == 0)\n",
    "print 'Count not pass:', sum(y_train_sub == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train_sub\n",
    "y_train = y_train_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can't GridSearchCV because I want the predicted probabilities for each combination of hyperparameters,\n",
    "# GridSearchCV won't give this to me\n",
    "# so apologies for the copy and pasted code\n",
    "# i wrote naive_model to return the predicted fail probabilities so Luke can throw them togther in his score function\n",
    "\n",
    "\n",
    "# approach: train on 2011-2014, validate on 2015 using log loss to select best model per class, test on 2016 to select best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a bunch of models for visualization without selecting best model on log loss basis -- skip for site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_hats = {}\n",
    "# here's how im gonna store this shit\n",
    "# p_hats['model'] = [0.1,0.11,0.2,0.22]\n",
    "# output = pd.DataFrame(p_hats)\n",
    "# output.set_index(test['inspection_date'])\n",
    "# output.to_csv('pred_prob_for_fails.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_model(clf_instance):\n",
    "    clf_instance.fit(x_train, y_train)\n",
    "    print 'accuracy:', round(clf_instance.score(x_test, y_test), 2)\n",
    "    p_hat = clf_instance.predict_proba(x_test)\n",
    "\n",
    "    print '\\n', 'Confusion matrix:'\n",
    "    conf = confusion_matrix(y_test, clf_instance.predict(x_test))\n",
    "    conf = conf / float(conf.sum())\n",
    "    print conf\n",
    "\n",
    "    print 'log loss:', round(score(y_test, p_hat), 2)\n",
    "    return map(lambda t: t[1], p_hat) # p_hat returned from sklearn is a list of lists with p for both classes, we want p for class 1 or fail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logreg\n",
    "logreg = LogReg(C = 1.0, class_weight = 'balanced') \n",
    "naive_model(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = map(lambda x: 10**x, np.linspace(-2, 2, 5))\n",
    "logreg_hyperparams = {'C': alphas}  \n",
    "logreg_labels = ['LogReg, C = 0.10','LogReg, C = 0.1','LogReg, C = 1','LogReg, C = 10','LogReg, C = 100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, alpha in enumerate(alphas):\n",
    "    logreg = LogReg(C = alpha, class_weight = 'balanced') \n",
    "    p_hats[logreg_labels[i]] = naive_model(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda\n",
    "lda = LDA() \n",
    "naive_model(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shrinkage_params = np.linspace(0.0, 1.0, 5)\n",
    "# lda_hyperparams = {'shrinkage': shrinkage}  \n",
    "lda_labels = ['LDA, shrinkage = 0.0','LDA, shrinkage = 0.25','LDA, shrinkage = 0.50','LDA, shrinkage = 0.75','LDA, shrinkage = 1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, shrinkage_param in enumerate(shrinkage_params):\n",
    "    lda = LDA(shrinkage = shrinkage_param, solver = 'lsqr') \n",
    "    p_hats[lda_labels[i]] = naive_model(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# qda\n",
    "qda = QDA() \n",
    "naive_model(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_params = np.linspace(0.0, 1.0, 5)\n",
    "qda_labels = ['QDA, reg = 0.0','QDA, reg = 0.25','QDA, reg = 0.50','QDA, reg = 0.75','QDA, reg = 1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, reg_param in enumerate(reg_params):\n",
    "    qda = QDA(reg_param = reg_param) \n",
    "    p_hats[qda_labels[i]] = naive_model(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "knn = KNN(n_neighbors = 5)\n",
    "naive_model(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns = [5,10,25,50]\n",
    "knn_labels = ['KNN, n = 5','KNN, n = 10','KNN, n = 25','KNN, n = 50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, n in enumerate(ns):\n",
    "    knn = KNN(n_neighbors = n) \n",
    "    p_hats[knn_labels[i]] = naive_model(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bagging - rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bagging - rf\n",
    "# non-cross validated\n",
    "rf = RandomForest()\n",
    "naive_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape[1] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validated\n",
    "rf_hyperparams = {'n_estimators': [25,50],\n",
    "                  'max_features': [x_train.shape[1] / 2, x_train.shape[1]], \n",
    "                  'max_depth': [2, 4]}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfs = [(25,x_train.shape[1] / 2),\n",
    "       (25,x_train.shape[1]), \n",
    "       (50,x_train.shape[1] / 2), \n",
    "       (50,x_train.shape[1])]\n",
    "\n",
    "rf_labels = ['RF, n = 25, features = 45','RF, n = 25, features = 90','RF, n = 50, features = 45','RF, n = 50, features = 90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, rf in enumerate(rfs):\n",
    "    rf = RandomForest(n_estimators = rf[0], max_features = rf[1]) \n",
    "    p_hats[rf_labels[i]] = naive_model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boosting - adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# boosting - adaboost\n",
    "# non-cross validated\n",
    "ada = AdaBoost()\n",
    "naive_model(ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adas = [25,50,100]\n",
    "\n",
    "ada_labels = ['AdaBoost, n = 25','AdaBoost, n = 50','AdaBoost, n = 100',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, ada in enumerate(adas):\n",
    "    adaboost = RandomForest(n_estimators = ada) \n",
    "    p_hats[ada_labels[i]] = naive_model(adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_hats['inspection_date'] = test['inspection_date']\n",
    "p_hats['result_binary'] = test['result_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save this down\n",
    "output = pd.DataFrame(p_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv('/Users/jeremywelborn1/Documents/Jeremy/Harvard/Classes/III_Junior/1st_CS109a/fggw/pred_prob_for_fails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# svm \n",
    "# non-cross validated\n",
    "# svc = SVC(probability = True)\n",
    "# pred_prob_svc = naive_model(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SLOW\n",
    "# grid CV\n",
    "Cs = map(lambda t: 10**t, range(0, 4))\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "svc_hyperparams = {'C': Cs, 'kernel': kernels}\n",
    "\n",
    "svc = SVC()\n",
    "gs_svc = GridSearchCV(svc, param_grid = svc_hyperparams)\n",
    "gs_svc.fit(x_train, y_train)\n",
    "p_hat = gs_clf.best_estimator_.predict_proba(x_test)\n",
    "\n",
    "print score(y_test, p_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Training a bunch of models and selecting best model on log loss basis -- for final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = map(lambda x: 10**x, np.linspace(-4, 4, 9))\n",
    "logreg_hyperparams = {'C': alphas}  \n",
    "\n",
    "shrinkage_params = np.linspace(0.0, 1.0, 5)\n",
    "lda_hyperparams = {'shrinkage': shrinkage_params, 'solver': ['lsqr']}  \n",
    "\n",
    "reg_params = np.linspace(0.0, 1.0, 5)\n",
    "qda_hyperparams = {'reg_param': reg_params}\n",
    "\n",
    "knn_hyperparams = {'n_neighbors': [5,10,25,50]}\n",
    "\n",
    "rf_hyperparams = {'n_estimators': [25,50],\n",
    "                  'max_features': [x_train.shape[1] / 2, x_train.shape[1]], \n",
    "                  'max_depth': [2, 4]}  \n",
    "\n",
    "ada_hyperparams = {'n_estimators': [25,50,100]} \n",
    "\n",
    "model_classes = [LogReg, LDA, QDA, KNN, RandomForest, AdaBoost]\n",
    "model_classes_str = ['LogReg', 'LDA', 'QDA', 'KNN', 'RandomForest', 'AdaBoost']\n",
    "model_class_hyperparams = [logreg_hyperparams, lda_hyperparams, qda_hyperparams, knn_hyperparams, rf_hyperparams, ada_hyperparams]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return best predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_hats_cv = {} # contains predicted probabilities for each of best models per model class \n",
    "clfs = [] # contains best model per model class\n",
    "\n",
    "for i, model_class in enumerate(model_classes):\n",
    "    gs_clf = GridSearchCV(model_class(), param_grid = model_class_hyperparams[i], scoring = 'neg_log_loss', cv = 5)\n",
    "    gs_clf.fit(x_train, y_train)\n",
    "    best_clf = gs_clf.best_estimator_\n",
    "    \n",
    "    score = 'log loss: ' + str(round(-1 * gs_clf.best_score_, 2))\n",
    "    \n",
    "    # best predictor label - class, score, and the tuned hyperparameters of interest \n",
    "    best_pred_label = model_classes_str[i] + ', ' + score + ', params = ' + \\\n",
    "                    repr([key + ': ' + str(gs_clf.best_estimator_.get_params()[key]) for key in model_class_hyperparams[i].keys()])\n",
    "    \n",
    "    # best predictor's predicted probabilities for class 1 fail \n",
    "    best_pred_prob = map(lambda t: t[1], best_clf.predict_proba(x_test))    \n",
    "    \n",
    "    # store best predictor's probabilities \n",
    "    p_hats_cv[best_pred_label] = best_pred_prob\n",
    "    \n",
    "    # store classifiers as well\n",
    "    clfs.append(best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search selected models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"KNN, log loss: 17.24, params = ['n_neighbors: 50']\",\n",
       " \"RandomForest, log loss: 0.94, params = ['n_estimators: 50', 'max_features: 45', 'max_depth: 2']\",\n",
       " \"QDA, log loss: 1.07, params = ['reg_param: 0.25']\",\n",
       " \"AdaBoost, log loss: 0.73, params = ['n_estimators: 100']\",\n",
       " \"LogReg, log loss: 0.75, params = ['C: 1.0']\",\n",
       " \"LDA, log loss: 0.72, params = ['shrinkage: 0.0', 'solver: lsqr']\"]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'Grid search selected models:'\n",
    "p_hats_cv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_hats_cv['inspection_date'] = test['inspection_date']\n",
    "p_hats_cv['result_binary'] = test['result_binary']\n",
    "output = pd.DataFrame(p_hats_cv)\n",
    "output.to_csv('/Users/jeremywelborn1/Documents/Jeremy/Harvard/Classes/III_Junior/1st_CS109a/fggw/pred_prob_for_fails_cv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.10000000000000001, 1.0, 10.0, 100.0, 1000.0, 10000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='neg_log_loss',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with this \n",
    "# gs_clf = GridSearchCV(LogReg(), param_grid = logreg_hyperparams, scoring = 'neg_log_loss', cv = 5)\n",
    "# gs_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use to get best n classifiers rather than best classifier \n",
    "#\n",
    "# takes a fit grid searched classifier and returns best n classifiers' hyperparameters\n",
    "# def best_params(gs_clf, n): \n",
    "#     best_ids = np.array(map(lambda t: t[1], np.array(gs_clf.grid_scores_))).argsort()[::-1][:n]\n",
    "#     best_params = [gs_clf.grid_scores_[best_id][0] for best_id in best_ids]\n",
    "#     return best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
