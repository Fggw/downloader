{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Imputation and Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data_built_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dba_name\n",
      "facility_type\n",
      "inspection_date\n",
      "inspection_type\n",
      "inspection_id\n",
      "violations\n",
      "latitude\n",
      "license_\n",
      "longitude\n",
      "results\n",
      "risk\n",
      "zip\n",
      "police_district\n",
      "precinct\n",
      "account_number\n",
      "site_number\n",
      "ward_precinct\n",
      "business_activity_Consumption of Liquor on Premises\n",
      "business_activity_Preparation of Food and Dining on Premise With Seating\n",
      "business_activity_Retail Sale of Tobacco\n",
      "business_activity_Retail Sales of Packaged Liquor\n",
      "business_activity_Retail Sales of Perishable Foods\n",
      "creation_date\n",
      "num_sanitation_in_police_dist\n",
      "index\n",
      "district\n",
      "crime_count_by_district\n",
      "date\n",
      "near_x\n",
      "near_y\n",
      "point_crime_count\n",
      "point_sanit_count\n",
      "result_binary\n",
      "grocery\n",
      "Canvass\n",
      "License\n",
      "Canvass Re-Inspection\n",
      "Complaint\n",
      "License Re-Inspection\n",
      "Short Form Complaint\n",
      "Complaint Re-Inspection\n",
      "Suspected Food Poisoning\n",
      "Consultation\n",
      "License-Task Force\n",
      "TMAX\n",
      "TMAX_3DayAvg\n",
      "TMAX_10DayAvg\n",
      "TMAX_30DayAvg\n",
      "violations_count\n",
      "citations_count\n",
      "critical_count\n",
      "serious_count\n",
      "minor_count\n",
      "corrected_count\n",
      "previous_count\n",
      "previous_fraction\n",
      "previous_result\n",
      "time_since_last_inspection\n",
      "previous_violations\n",
      "previous_citations\n",
      "previous_critical\n",
      "previous_serious\n",
      "previous_minor\n",
      "previous_corrected\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns.values:\n",
    "    print col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120308, 38)\n",
      "(120308, 64)\n",
      "(61493, 38)\n"
     ]
    }
   ],
   "source": [
    "cols = ['inspection_date',\n",
    "        'facility_type',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'results',\n",
    "        'risk',\n",
    "        'business_activity_Consumption of Liquor on Premises',\n",
    "        'business_activity_Preparation of Food and Dining on Premise With Seating',\n",
    "        'business_activity_Retail Sale of Tobacco',\n",
    "        'business_activity_Retail Sales of Packaged Liquor',\n",
    "        'business_activity_Retail Sales of Perishable Foods',\n",
    "        'Canvass', # inspection_types\n",
    "        'License',\n",
    "        'Canvass Re-Inspection',\n",
    "        'Complaint',\n",
    "        'License Re-Inspection',\n",
    "        'Short Form Complaint',\n",
    "        'Complaint Re-Inspection',\n",
    "        'Suspected Food Poisoning',\n",
    "        'Consultation',\n",
    "        'License-Task Force',\n",
    "        'point_crime_count',\n",
    "        'point_sanit_count',\n",
    "        'result_binary',\n",
    "        'TMAX',\n",
    "        'TMAX_3DayAvg',\n",
    "        'TMAX_10DayAvg',\n",
    "        'TMAX_30DayAvg',\n",
    "        'previous_count',\n",
    "        'previous_fraction',\n",
    "        'previous_result',\n",
    "        'time_since_last_inspection',\n",
    "        'previous_violations',\n",
    "        'previous_citations',\n",
    "        'previous_critical',\n",
    "        'previous_serious',\n",
    "        'previous_minor',\n",
    "        'previous_corrected']\n",
    "\n",
    "print data[cols].shape\n",
    "print data.shape\n",
    "\n",
    "# select specific features\n",
    "data_sub = data[cols]\n",
    "# what would happen if we dropped NaNs\n",
    "print data_sub.dropna(axis=0, how='any').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub = data_sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temporary -- dumping correct crime data and sanitation complaint data into dataset\n",
    "data_dump = pd.read_csv('./data/crime_sanit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sub['crime'] = data_dump['crime'].copy()\n",
    "data_sub['sanit'] = data_dump['sanitation'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-04 00:00:00\n",
      "2016-12-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# convert string to datetime to be safe\n",
    "data_sub['inspection_date'] = pd.to_datetime(data_sub['inspection_date'])\n",
    "\n",
    "print data_sub['inspection_date'].min()\n",
    "print data_sub['inspection_date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation and encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inspection_date 0\n",
      "1 facility_type 640\n",
      "2 latitude 460\n",
      "3 longitude 460\n",
      "4 results 0\n",
      "5 risk 38\n",
      "6 business_activity_Consumption of Liquor on Premises 7205\n",
      "7 business_activity_Preparation of Food and Dining on Premise With Seating 7205\n",
      "8 business_activity_Retail Sale of Tobacco 7205\n",
      "9 business_activity_Retail Sales of Packaged Liquor 7205\n",
      "10 business_activity_Retail Sales of Perishable Foods 7205\n",
      "11 Canvass 0\n",
      "12 License 0\n",
      "13 Canvass Re-Inspection 0\n",
      "14 Complaint 0\n",
      "15 License Re-Inspection 0\n",
      "16 Short Form Complaint 0\n",
      "17 Complaint Re-Inspection 0\n",
      "18 Suspected Food Poisoning 0\n",
      "19 Consultation 0\n",
      "20 License-Task Force 0\n",
      "21 point_crime_count 2208\n",
      "22 point_sanit_count 18266\n",
      "23 result_binary 0\n",
      "24 TMAX 17440\n",
      "25 TMAX_3DayAvg 17440\n",
      "26 TMAX_10DayAvg 17732\n",
      "27 TMAX_30DayAvg 18690\n",
      "28 previous_count 0\n",
      "29 previous_fraction 26924\n",
      "30 previous_result 26924\n",
      "31 time_since_last_inspection 26924\n",
      "32 previous_violations 33687\n",
      "33 previous_citations 33687\n",
      "34 previous_critical 33687\n",
      "35 previous_serious 33687\n",
      "36 previous_minor 33687\n",
      "37 previous_corrected 33687\n",
      "38 crime 4\n",
      "39 sanit 4\n",
      "\n",
      "\n",
      "\n",
      "Total cells in dataset: 4812320\n",
      "Total cells containing NaN in dataset: 412301\n",
      "Ratio of NaN to total datapoints: 0.09\n"
     ]
    }
   ],
   "source": [
    "# non-KNN approach:\n",
    "# replace null values with feature mean (quantitative) or feature mode (qualitative)\n",
    "# for column in data_sub.columns:\n",
    "#     if (data_sub[column].unique().shape[0] < 8) or (data_sub[column].dtype == np.dtype('object')):\n",
    "#         data_sub[column].fillna(value = data_sub[column].mode()[0], inplace = True)\n",
    "#     else:\n",
    "#         data_sub[column].fillna(value = data_sub[column].mean(), inplace = True)\n",
    "\n",
    "# check no null values\n",
    "nan_cnt = 0\n",
    "for i, column in enumerate(data_sub.columns):\n",
    "    print i, column, np.array(data_sub[column].isnull()).sum()\n",
    "    nan_cnt = nan_cnt + np.array(data_sub[column].isnull()).sum()\n",
    "\n",
    "print '\\n\\n'\n",
    "print 'Total cells in dataset:', data_sub.size\n",
    "print 'Total cells containing NaN in dataset:', nan_cnt\n",
    "print 'Ratio of NaN to total datapoints:', round(nan_cnt / float(data_sub.size),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how should we handle the historical count data?\n",
    "# there's no optimal solution: either (1) impute, biasing that they wouldn't have done well \n",
    "# or (2) set to 0, biasing that they would've done well? We'll set to 0 for simplicity.\n",
    "\n",
    "print 'count of obs for which previous_count == 0:', data_sub[data_sub['previous_count'] == 0.0].shape[0]\n",
    "print 'this corresponds with number of nans for previous_fraction, previous_result, time_since_last_inspection, which is what we\\'d expect'\n",
    "\n",
    "# set all these to 0 then\n",
    "hist_cols = ['previous_fraction',\n",
    "            'previous_result',\n",
    "            'time_since_last_inspection',\n",
    "            'previous_violations',\n",
    "            'previous_citations',\n",
    "            'previous_critical',\n",
    "            'previous_serious',\n",
    "            'previous_minor',\n",
    "            'previous_corrected']\n",
    "\n",
    "for hist_col in hist_cols:\n",
    "    data_sub.loc[data_sub['previous_count'] == 0.0, hist_col] = data_sub.loc[data_sub['previous_count'] == 0.0, hist_col].fillna(value = 0)\n",
    "    \n",
    "print '\\n here are NaN counts after this step: \\n'\n",
    "for i, column in enumerate(data_sub.columns):\n",
    "    print i, column, np.array(data_sub[column].isnull()).sum()\n",
    "    \n",
    "print '37-42 still have a uniform number of null values because they had no text from the inspection, so nothing to scrape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obs with no null values\n",
    "print 'count of obs with no null values:', data_sub.shape[0] - data_sub.isnull().any(axis = 1).sum()\n",
    "print 'count of features with no null values:', data_sub.shape[1] - data_sub.isnull().any(axis = 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# features with no null values we can use to impute\n",
    "data_sub.isnull().any(axis = 0)[data_sub.isnull().any(axis = 0) == False].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so that the imputer has enough predictors, to work with, fill in risk by mean / mode approach since only 63 obs have nulls\n",
    "\n",
    "data_sub['risk'].fillna(value = data_sub['risk'].mode()[0], inplace = True)\n",
    "\n",
    "# same\n",
    "# data_sub['inspection_type'].fillna(value = data_sub['inspection_type'].mode()[0], inplace = True)\n",
    "\n",
    "# check\n",
    "print 'number of null values in risk:', data_sub['risk'].isnull().sum()\n",
    "# print 'number of null values in inspection_type:', data_sub['inspection_type'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN \n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have to encode the data first... but facility_type is annoying to do...\n",
    "print data_sub.facility_type.value_counts()[0:10]\n",
    "\n",
    "take_types = data_sub.facility_type.value_counts()[0:50].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, row in data_sub[['facility_type']].iterrows():\n",
    "    if row.values not in take_types:\n",
    "        data_sub.loc[i, 'facility_type'] = 'Other'\n",
    "    else:\n",
    "        continue\n",
    "    if (i % 500 == 0):\n",
    "        print 'finished iteration:', i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check response\n",
    "print 'Useless response count:', sum((data_sub.results != 'Pass') & (data_sub.results != 'Pass w/ Conditions') & (data_sub.results != 'Fail'))\n",
    "indices = (data_sub.results == 'Pass') | (data_sub.results == 'Pass w/ Conditions') | (data_sub.results == 'Fail')\n",
    "data_sub = data_sub[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# risk\n",
    "indices = (data_sub.risk == 'Risk 1 (High)') | (data_sub.risk == 'Risk 2 (Medium)') | (data_sub.risk == 'Risk 3 (Low)')\n",
    "data_sub = data_sub[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sub = data_sub.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have to these encode first\n",
    "data_sub_encoded = pd.DataFrame({})\n",
    "for column in data_sub.columns:\n",
    "#     if (data_sub[column].unique().shape[0] < 8) or data_sub[column].dtype == np.dtype('object'):\n",
    "    if (data_sub[column].dtype == np.dtype('object')) & (column not in ['results']):\n",
    "        encoding = pd.get_dummies(data_sub[column])\n",
    "        data_sub_encoded = pd.concat([data_sub_encoded, encoding], axis = 1)\n",
    "    else:\n",
    "#         data_sub_encoded = pd.concat([data_sub_encoded, data_sub[[column]].astype(float)], axis = 1)\n",
    "        data_sub_encoded = pd.concat([data_sub_encoded, data_sub[[column]]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train data are observations that are complete\n",
    "# test data (which we wish to impute) are observations that are incomplete\n",
    "train = data_sub_encoded[np.logical_not(data_sub_encoded.isnull().any(axis = 1).values)]\n",
    "test = data_sub_encoded[data_sub_encoded.isnull().any(axis = 1).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset indices because we've dropped rows, NearestNeighbors was returning wrong indices because they weren't reset\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dont actually have to split x and y since we specify what features we're imputing on in impute_cols\n",
    "# x_train = train.drop(['result_binary', 'results'], axis = 1) \n",
    "# x_test = test.drop(['result_binary', 'results'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sub.isnull().any(axis = 0)[data_sub.isnull().any(axis = 0) == False].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impute_cols = ['Risk 1 (High)', 'Risk 2 (Medium)', 'Risk 3 (Low)', 'previous_count', 'previous_fraction',\n",
    "       'previous_result', 'time_since_last_inspection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors = 5)\n",
    "knn.fit(train[impute_cols]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return k nearest neighbors based on features we have for all observations\n",
    "# then fill in other values using these nearest neighbors \n",
    "\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "for i, row in test.iterrows(): # for each observation with incomplete features\n",
    "    ind = knn.kneighbors(X = row[impute_cols].values.reshape(1,-1), return_distance=False)[0] # return indices of nearest neighbors with complete features\n",
    "    for col in test.columns.values[row.isnull().values]: # for each feature that're NaNs    \n",
    "        train_vals = np.array(train.loc[ind, col]) # get vals from nearest neighbors for this col\n",
    "        if train[col].unique().shape[0] <= 2: # if indicator \n",
    "            test.loc[i, col] = stats.mode(train_vals)[0][0] # fill w/ mode\n",
    "        else:\n",
    "            test.loc[i, col] = train_vals.mean() # fill w/ mean\n",
    "    if (i % 500 == 0):\n",
    "        print 'finished iteration:', i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this won't work, apply flattens the thing\n",
    "# import math\n",
    "# from scipy import stats\n",
    "\n",
    "# def impute_row(row):\n",
    "#     ind = knn.kneighbors(X = row[impute_cols].values.reshape(1, -1), return_distance=False)[0]\n",
    "#     for col in test.columns.values[row.isnull().values]: # for each feature that're NaNs    \n",
    "#         train_vals = np.array(train.loc[ind, col])\n",
    "#         if train[col].unique().shape[0] <= 2:\n",
    "#             row[col] = stats.mode(train_vals)[0][0]\n",
    "#         else:\n",
    "#             row[col] = train_vals.mean()\n",
    "\n",
    "# test = test.apply(impute_row, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ready = pd.concat([train, test], axis = 0)\n",
    "data_ready = data_ready.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so don't have to rerun the code above\n",
    "data_ready.to_csv('./data/data_ready.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
